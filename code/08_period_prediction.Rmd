---
title: "Period-style prediction"
author: Casey Breen
---


Overall: 

```{r}
## library packages
library(here)
library(data.table)
library(knitr)
library(kableExtra)
library(tidyverse)
library(origami)
library(SuperLearner)
library(sl3)
library(doParallel)
library(ggpubr)
library(cowplot)
library(caret)

## source custom functions 
source("helpers.R")
```


```{r}
## Load and clean data
data <- fread("~/workspace/data/v3/censoc_dmf_v3_linked.csv") %>%
  janitor::clean_names() %>%
  filter(byear %in% 1880:1920) 

data <- data %>% 
  filter(dyear >= 1975) %>%  
  mutate(death_dummy = case_when(
    dyear %in% 1975:1984 ~ 1,
    TRUE ~ 0)) 

```


```{r}
data <- recode_education(data, educ_var = "educd")

recode_all_variables <- function(data) {
  data %>%
    # Recode Wage Income
    mutate(incwage_recode = case_when(
      incwage == 999998 ~ 0,
      TRUE ~ incwage
    )) %>%
     # Recode Wage Income
    mutate(educ_yrs = case_when(
     is.na(educ_yrs) ~ 0,
      TRUE ~ educ_yrs
    )) %>%
    # Recode Socioeconomic Index
    mutate(sei_recode = case_when(
      sei == 0 ~ 0,
      TRUE ~ sei
    )) %>%
    # Recode Occupational Score
    mutate(occscore_recode = case_when(
      occscore == 0 ~ 0,
      TRUE ~ occscore
    )) %>%
    # Recode Number of Persons in Household
    mutate(numperhh_recode = pmin(numperhh, 20)) %>%
    # Recode Prestige Score
    mutate(presgl_recode = case_when(
      presgl == 0 ~ 0,
      TRUE ~ presgl
    )) %>%
    # Recode Race
    mutate(
      race_white_recode = as.integer(race == 1),
      race_black_recode = as.integer(race == 2),
      race_other_recode = as.integer(race %in% 3:10)
    ) %>%
    # Recode Metropolitan Status
    mutate(
      metro_nometro_recode = as.integer(metro == 1),
      metro_city_recode = as.integer(metro == 2),
      metro_suburb_recode = as.integer(metro == 3)
    ) %>%
    # Recode Marital Status
    mutate(
      marriage_married_recode = as.integer(marst %in% c(1, 2)),
      marriage_divorced_recode = as.integer(marst %in% c(3, 4)),
      marriage_widowed_recode = as.integer(marst == 5),
      marriage_single_recode = as.integer(marst == 6)
    ) %>%
    # Recode Renter Status
    mutate(renter_recode = as.integer(rent > 0)) %>%
    # Recode Hispanic Status
    mutate(hispan_recode = as.integer(hispan > 0)) %>%
    # Recode Class Worker
    mutate(
      classwkr_self_employed_recode = as.integer(classwkr == 1),
      classwkr_wage_worker_recode = as.integer(classwkr == 2)
    ) %>%
    # Recode Number of Families
    mutate(nfams = pmin(nfams, 10)) %>%
    # Lives with Mother Recode
    mutate(momloc_self_employed_recode = as.integer(momloc > 0)) %>%
    # Migration Recode
    mutate(
      migrate5_same_house_recode = as.integer(migrate5 == 1),
      migrate5_state_move_recode = as.integer(migrate5 == 2),
      migrate5_country_move_recode = as.integer(migrate5 == 3),
      migrate5_international_move_recode = as.integer(migrate5 == 4)
    ) %>%
    # Urban Recode
    mutate(urban_recode = as.integer(urban == 2)) %>%
    # Labor Force Recode
    mutate(labforce_recode = as.integer(labforce == 2)) %>%
    # Farm Recode
    mutate(farm_recode = as.integer(farm == 2)) %>%
    # Homeowner Recode
    mutate(homeowner_recode = as.integer(ownershpd == 10)) %>%
    # Multiple Generations Recode
    mutate(
      multgend_1gen_recode = as.integer(multgend == 10),
      multgend_2gen_recode = as.integer(multgend %in% 20:23),
      multgend_3gen_recode = as.integer(multgend %in% 30:32)
    ) %>%
    # Employment Status Recode
    mutate(empstatd_employed_recode = as.integer(empstatd %in% 10:13))
}
```
 

```{r}
# Example usage
data <- recode_all_variables(data) %>% 
  # drop_na() %>% 
  sample_n(100000)
```



```{r}
data <- data %>% 
  select(histid, nsibs, rent, renter_recode, hispan_recode, classwkr, nfams, nmothers, nfathers, ncouples, wkswork1, hrswork1, death_age, educ_yrs, incwage_recode, sei_recode, occscore_recode, numperhh_recode, presgl_recode, race_white_recode, race_black_recode, race_other_recode, metro_nometro_recode, metro_city_recode, metro_suburb_recode, marriage_married_recode, marriage_divorced_recode, marriage_widowed_recode, marriage_single_recode, classwkr_self_employed_recode, classwkr_wage_worker_recode, momloc_self_employed_recode, migrate5_same_house_recode, migrate5_state_move_recode, migrate5_country_move_recode, migrate5_international_move_recode, urban_recode, labforce_recode, farm_recode, homeowner_recode, multgend_1gen_recode, multgend_2gen_recode, multgend_3gen_recode, empstatd_employed_recode, byear,  statefip, countyicp, death_dummy) 

## normalize numeric variable 
data <- data %>%
   mutate(across(!death_age & !histid & !statefip & !countyicp & !educ_yrs
                 & !statefip & !countyicp & !educ_yrs & !sei_recode & !occscore_recode & !presgl_recode & !incwage_recode,
                 normalize))
```

```{r}
colSums(is.na(data))
```



## split up sample 

```{r}
## Split data into training and holdout sets
set.seed(123)  # Set seed for reproducibility in sample splitting

train_indices <- sample(seq_len(nrow(data)), size = .75 * nrow(data))


train_data <- data[train_indices, ]
holdout_data <- data[-train_indices, ]

data
```

## Basic logistic regression 

```{r}
## Constructing a formula with all covariates
covariates <- setdiff(names(train_data), c("death_dummzy", "histid", "death_age"))  # Assuming 'histid' is an ID variable and 'death_dummy' is the outcome

## Create the model formula dynamically
model_formula <- reformulate(termlabels = covariates, response = "death_dummy")

# Fitting the model
model <- glm(model_formula,
             data = train_data, family = "binomial")

# Fitting the model
model2 <- glm(death_dummy ~ as.factor(byear),
             data =  train_data, family = "binomial")

# Generating predictions as probabilities
prob_predictions1 <- predict(model, newdata = holdout_data , type = "response")

# Generating predictions as probabilities
prob_predictions2 <- predict(model2, newdata = holdout_data , type = "response")


holdout_data_binary <- holdout_data %>%
  mutate(prob_predictions1_binary = ifelse(prob_predictions1 > 0.5, 1, 0),
         prob_predictions2_binary = ifelse(prob_predictions2 > 0.5, 1, 0))
#

holdout_data <- holdout_data %>%
  mutate(prob_predictions1 = prob_predictions1,
         prob_predictions2 = prob_predictions2)

calculate_mcc(data = holdout_data_binary %>% select(death_dummy, prob_predictions1_binary, prob_predictions2_binary),
                             outcome_var = "death_dummy")

calculate_r_squared_for_probabilities(data = holdout_data %>% select(death_dummy, prob_predictions1, prob_predictions2),
                             outcome_var = "death_dummy")
```

## ML Stuff 



```{r}
# create the task (i.e., use washb_data to predict outcome using covariates)
task <- make_sl3_Task(
  data = train_data,
  outcome = "death_dummy",
  covariates = colnames(train_data %>% select(-histid, -death_dummy, -death_age))
)

# let's examine the task
task
```

## model with birth years 


```{r}
options(sl3.verbose = TRUE)


df <- train_data %>% 
  select(-histid, -death_age) 


# make task
chspred_task <- make_sl3_Task(
  data = df,
  covariates = colnames(df %>% select(-death_dummy)),
  outcome = "death_dummy"
)

# make learners
glm_learner <- Lrnr_glm$new()
lasso_learner <- Lrnr_glmnet$new(alpha = 1)
ridge_learner <- Lrnr_glmnet$new(alpha = 0)
enet_learner <- Lrnr_glmnet$new(alpha = 0.5)
# curated_glm_learner uses formula = "mi ~ smoke + beta"
# curated_glm_learner <- Lrnr_glm_fast$new(covariates = c("smoke", "beta"))
mean_learner <- Lrnr_mean$new() # That is one mean learner!
glm_fast_learner <- Lrnr_glm_fast$new()
ranger_learner <- Lrnr_ranger$new()
svm_learner <- Lrnr_svm$new()
xgb_learner <- Lrnr_xgboost$new()

# screening
screen_cor <- make_learner(Lrnr_screener_correlation)
glm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)

# stack learners together
stack <- make_learner(
  Stack,
  glm_pipeline, glm_learner,
  lasso_learner, ridge_learner, enet_learner,
  mean_learner, glm_fast_learner,
  ranger_learner, xgb_learner
)

# make and train SL
sl <- Lrnr_sl$new(
  learners = stack
)
sl_fit <- sl$train(chspred_task)

sl_fit$cv_risk(loss_squared_error)
```


```{r}
prediction_task <- make_sl3_Task(
  data = holdout_data,
  covariates = holdout_data %>% select(-histid, -death_age, -death_dummy) %>% colnames(),
  outcome = "death_dummy",
  outcome_type = "binary")

## make predictions
df_pred <- data.frame(
  glm = sl_fit$learner_fits$Lrnr_glm_fast_TRUE_Cholesky$predict(task = prediction_task),
  mean = sl_fit$learner_fits$Lrnr_mean$predict(task = prediction_task),
  glm_enet = sl_fit$learner_fits$Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE$predict(task = prediction_task),
  glm_ridge = sl_fit$learner_fits$Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE$predict(task = prediction_task),
  glm_lasso = sl_fit$learner_fits$Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE$predict(task = prediction_task),
  screen_glm = sl_fit$learner_fits$`Pipeline(Lrnr_screener_correlation_pearson_c("rank", "threshold")_5_0.1_2->Lrnr_glm_TRUE)`$predict(task = prediction_task),
  xgboost = sl_fit$learner_fits$Lrnr_xgboost_20_1$predict(task = prediction_task),
  ranger = sl_fit$learner_fits$Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE$predict(task = prediction_task),  
#  gbm = sl_fit$learner_fits$Lrnr_gam_NULL_NULL_GCV.Cp$predict(task = prediction_task),
  Superlearner = sl_fit$predict(task = prediction_task),
  death_dummy = holdout_data$death_dummy)

# holdout 
df_binary <- df_pred %>% 
  mutate(glm = ifelse(glm > 0.5, 1, 0),
         mean = ifelse(mean > 0.5, 1, 0),
         glm_enet = ifelse(glm_enet > 0.5, 1, 0),
         glm_ridge = ifelse(glm_ridge > 0.5, 1, 0),
         glm_lasso = ifelse(glm_lasso > 0.5, 1, 0),
         screen_glm = ifelse(screen_glm > 0.5, 1, 0),
         xgboost = ifelse(xgboost > 0.5, 1, 0),
         ranger = ifelse(ranger > 0.5, 1, 0),
         Superlearner = ifelse(Superlearner > 0.5, 1, 0)) 

# Example usage:
outcome_variable <- "death_dummy"  # specify the outcome variable
mcc_results <- calculate_mcc(data = df_binary,
                             outcome_var = outcome_variable)

accuracy_results <- calculate_accuracy(data = df_binary,
                             outcome_var = outcome_variable)


rsquared_results <- calculate_r_squared_for_probabilities(data = df_pred %>% select(-mean),
                                                           outcome_var = outcome_variable)

```



```{r}
mcc_results %>% 
  ggplot(aes(x = reorder(variable, -mcc),
             y = mcc,
             label = paste0(round(mcc, 3)))) + 
  geom_col(color = "black", fill  = "grey") + 
  geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 4) + 
 # scale_y_continuous(labels = scales::percent, limits = c(0, 0.015)) + 
  theme_cowplot() + 
  labs(x = "Algorithm",
       y = expression ("Mathews Correlation Coefficient"),
       title = bquote(bold("Algorithm Performance ")))
```

```{r}
predicted_prob_death <- df_pred %>% 
  mutate(decile = cut_number(Superlearner, 100)) %>% 
  group_by(decile) %>% 
 # mutate(prediction = round(Superlearner, 1)) %>%
  # mutate(prediction = cut_number(Superlearner, 100)) %>% 
  mutate(binary_pred = ifelse(Superlearner > 0.5, 1, 0)) %>% 
  group_by(decile) %>% 
  summarize(prediction = mean(Superlearner),
            proportion_died = mean(death_dummy),
            accuracy = mean(binary_pred == death_dummy),
            count = n()) %>% 
  mutate(percentile = row_number()) %>% 
  ggplot(aes(x = prediction, y = proportion_died, color = accuracy)) + 
  geom_point(alpha = 0.7, size =2 ) +
  stat_cor(method = "pearson", label.x = .60, label.y = 0.3) +
  theme_cowplot() +
  labs(y = "Observed proportion of deaths",
       x = "Predicted probability of death") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "darkgrey") + 
  ylim(0, 1) + 
  xlim(0, 1) +
  scale_color_gradient2(low = "red", high = "blue", mid = "purple", midpoint = 0.7, 
                        limits = c(0.4, 1), space = "Lab", name = "Accuracy")
```

# Model no byear 

```{r}
options(sl3.verbose = TRUE)


df <- train_data %>% 
  select(-histid, -death_age) 


# make task
chspred_task <- make_sl3_Task(
  data = df,
  covariates = colnames(df %>% select(-death_dummy, -byear)),
  outcome = "death_dummy"
)

# make learners
glm_learner <- Lrnr_glm$new()
lasso_learner <- Lrnr_glmnet$new(alpha = 1)
ridge_learner <- Lrnr_glmnet$new(alpha = 0)
enet_learner <- Lrnr_glmnet$new(alpha = 0.5)
# curated_glm_learner uses formula = "mi ~ smoke + beta"
# curated_glm_learner <- Lrnr_glm_fast$new(covariates = c("smoke", "beta"))
mean_learner <- Lrnr_mean$new() # That is one mean learner!
glm_fast_learner <- Lrnr_glm_fast$new()
ranger_learner <- Lrnr_ranger$new()
svm_learner <- Lrnr_svm$new()
xgb_learner <- Lrnr_xgboost$new()

# screening
screen_cor <- make_learner(Lrnr_screener_correlation)
glm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)

# stack learners together
stack <- make_learner(
  Stack,
   glm_pipeline, glm_learner,
  lasso_learner, ridge_learner, enet_learner,
   mean_learner, glm_fast_learner,
  ranger_learner, xgb_learner
)

# make and train SL
sl <- Lrnr_sl$new(
  learners = stack
)
sl_fit_nobyear <- sl$train(chspred_task)

sl_fit_nobyear$cv_risk(loss_squared_error)
```




```{r}
prediction_task_nobyear <- make_sl3_Task(
  data = holdout_data,
  covariates = holdout_data %>% select(-histid, -death_age, -death_dummy) %>% colnames(),
  outcome = "death_dummy",
  outcome_type = "binary")

## make predictions
df_pred_nobyear <- data.frame(
  glm = sl_fit_nobyear$learner_fits$Lrnr_glm_fast_TRUE_Cholesky$predict(task = prediction_task_nobyear),
  mean = sl_fit_nobyear$learner_fits$Lrnr_mean$predict(task = prediction_task_nobyear),
  glm_enet = sl_fit_nobyear$learner_fits$Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE$predict(task = prediction_task_nobyear),
  glm_ridge = sl_fit_nobyear$learner_fits$Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE$predict(task = prediction_task_nobyear),
  glm_lasso = sl_fit_nobyear$learner_fits$Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE$predict(task = prediction_task_nobyear),
  screen_glm = sl_fit_nobyear$learner_fits$`Pipeline(Lrnr_screener_correlation_pearson_c("rank", "threshold")_5_0.1_2->Lrnr_glm_TRUE)`$predict(task = prediction_task_nobyear),
  xgboost = sl_fit_nobyear$learner_fits$Lrnr_xgboost_20_1$predict(task = prediction_task_nobyear),
  ranger = sl_fit_nobyear$learner_fits$Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE$predict(task = prediction_task_nobyear),  
#  gbm = sl_fit$learner_fits$Lrnr_gam_NULL_NULL_GCV.Cp$predict(task = prediction_task),
  Superlearner = sl_fit_nobyear$predict(task = prediction_task_nobyear),
  death_dummy = holdout_data$death_dummy)

# holdout 
df_binary_nobyear <- df_pred_nobyear %>% 
  mutate(glm = ifelse(glm > 0.5, 1, 0),
         mean = ifelse(mean > 0.5, 1, 0),
         glm_enet = ifelse(glm_enet > 0.5, 1, 0),
         glm_ridge = ifelse(glm_ridge > 0.5, 1, 0),
         glm_lasso = ifelse(glm_lasso > 0.5, 1, 0),
         screen_glm = ifelse(screen_glm > 0.5, 1, 0),
         xgboost = ifelse(xgboost > 0.5, 1, 0),
         ranger = ifelse(ranger > 0.5, 1, 0),
         Superlearner = ifelse(Superlearner > 0.5, 1, 0)) 

# Example usage:
outcome_variable <- "death_dummy"  # specify the outcome variable
mcc_results_nobyear <- calculate_mcc(data = df_binary_nobyear,
                             outcome_var = outcome_variable)

mcc_results_nobyear <- calculate_mcc(data = df_binary_nobyear,
                             outcome_var = outcome_variable)

rsquared_results_nobyear <- calculate_r_squared_for_probabilities(data = df_pred_nobyear,
                             outcome_var = outcome_variable)

accuracy_results_nobyear <- calculate_accuracy(data = df_binary_nobyear,
                             outcome_var = outcome_variable)
```



```{r}
r_squared_df <- rsquared_results_nobyear %>%       
  mutate(predictors = "Sociodemographics") %>% 
  bind_rows(rsquared_results %>% 
              mutate(predictors = "Age + Sociodemographics")) %>%
  filter(variable == "Superlearner") %>% 
  bind_rows(tibble(r_squared = 0.1629944, predictors = "Age"))

r_squared_plot <- r_squared_df %>% 
ggplot(aes(x = reorder(predictors, -r_squared),
             y = r_squared,
             label = paste0(round(r_squared, 3)))) + 
  geom_col(color = "black", fill  = "grey") + 
  geom_text(position = position_dodge(width = .9),    # move to center of bars
              vjust = -0.5,    # nudge above top of bar
              size = 4) + 
 # scale_y_continuous(labels = scales::percent, limits = c(0, 0.015)) + 
  theme_cowplot() + 
  ylim(0, max(r_squared_df$r_squared) + 0.02) + 
  labs(x = "Algorithm",
       y = bquote(bold(R^2)))


period_prediction_plot <- cowplot::plot_grid(predicted_prob_death, NULL, r_squared_plot, nrow = 1, rel_widths = c(1, 0.1, 1), labels = c("A", "", "B"))

ggsave(period_prediction_plot, filename = here("figures/period_prediction_plot.png"), height = 4, width = 10)
```




